{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN v1.0",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthikB/Neural-Networks-from-Scratch-2/blob/NN-v2.0/Neural%20Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STSi9WjjQ6dF",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adP37luQEykF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "################## helper functions ######################\n",
        "\n",
        "# Generating a basic dataset.\n",
        "def create_data(total_samples, range_of_data):\n",
        "    X1, X2, Y = [], [], []\n",
        "\n",
        "    for datapoints in range(total_samples):\n",
        "        x2 = np.random.randint(1, range_of_data + 1)\n",
        "        x1 = np.random.randint(1, range_of_data + 1)\n",
        "\n",
        "        if x1 < range_of_data / 2:\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "        X1.append(x1)\n",
        "        X2.append(x2)\n",
        "        X = np.array([X1, X2])\n",
        "        Y.append(label)\n",
        "\n",
        "    return np.array(X).reshape(2, -1), np.array(Y).reshape(1, -1)\n",
        "\n",
        "# Defining Activation Functions\n",
        "def sigmoid(z):\n",
        "  A =  1 / (1 + np.exp(-z))\n",
        "  cache = z\n",
        "  return A, cache\n",
        "\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0, Z)\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu_derivative(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_derivative(dA, cache):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "    Z = cache\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    dZ = dA * s * (1 - s)\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def initialize_random_parameters(layer_dims, X):\n",
        "    parameters = {}\n",
        "    parameters[\"W1\"] = np.random.randn(layer_dims[0], X.shape[0]) * 0.01\n",
        "    parameters[\"b1\"] = np.zeros((layer_dims[0], 1))\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        parameters[\"W\" + str(i+1)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
        "        parameters[\"b\" + str(i+1)] = np.zeros((layer_dims[i], 1))\n",
        "\n",
        "    return parameters\n",
        "  \n",
        "# To check the accuracy of our Model.\n",
        "def accuracy_score(Yhat, Y):\n",
        "  Yhat = np.where(Yhat < 0.5, 0, 1)\n",
        "  accuracy = 100 - np.mean(np.abs(Yhat-Y) * 100)\n",
        "  return accuracy\n",
        "\n",
        "# To visualize our dataset/predictions.\n",
        "def plot_data(data, type_of_data):\n",
        "    X = data[0]\n",
        "    Y = data[1]\n",
        "    sns.scatterplot(X[0], X[1], hue=Y[0])\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.title(type_of_data)\n",
        "    type_of_data = 'Plots : ' + type_of_data + \".png\"\n",
        "    plt.savefig(type_of_data)\n",
        "    plt.close()\n",
        "\n",
        "    \n",
        "def plot_cost_function(epoch_log, cost_log):\n",
        "  plt.plot(epoch_log, cost_log)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(\"Cost\")\n",
        "  plt.title(\"Cost Function\")\n",
        "  plt.savefig('Plots : Cost Function.png')\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEdCM-3KQ1Aq",
        "colab_type": "text"
      },
      "source": [
        "### Basic Neuron Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq6Car8gb_ma",
        "colab_type": "code",
        "outputId": "d63ca71a-8222-4183-808f-a4083b68bc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Defining main Class.\n",
        "class Neuron:\n",
        "  \n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias   = bias\n",
        "    \n",
        "    \n",
        "  def feedforward(self, X):\n",
        "    # Weights inputs, add bias and then use activation function\n",
        "    math_e_magic = np.dot(self.weights, X) + self.bias\n",
        "    output, _ = sigmoid(math_e_magic)\n",
        "    \n",
        "    return output\n",
        "    \n",
        "    \n",
        "# Testing\n",
        "X = [2, 3]\n",
        "weights, bias = [2, 3], [4]\n",
        "\n",
        "# > Defining the Class\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "# > Feedforwarding\n",
        "print(neuron.feedforward(X))\n",
        "# Result --> array([0.99999996])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.99999996]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7dGIPW5obO",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2y30xcesil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from helpers import *\n",
        "\n",
        "class NeuralNetwork:    \n",
        "  \n",
        "  # Defining the Feedforward function\n",
        "  def feedforward(self, X, parameters, activation_used):\n",
        "\n",
        "    def linear_forward(A, W, b, activation):\n",
        "      Z = np.dot(W, A) + b\n",
        "      linear_cache = (A, W, b)\n",
        "\n",
        "      if activation == 'sigmoid':\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "      elif activation == 'relu':\n",
        "        A, activation_cache = relu(Z)\n",
        "      \n",
        "      # Saving some variables that will be needed later in Back Propagation in the form of caches. You don't need to understand them for now.\n",
        "      cache = (linear_cache, activation_cache)\n",
        "      return A, cache\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # Total number of Layers in our Network\n",
        "    \n",
        "    # Iterating over every Layer and computing the Activations.\n",
        "    # See how different activations are being used.\n",
        "    # You can play with the activations in the hidden layers, but the Output layer activation is always set to 'sigmoid'.\n",
        "    for i in range(1, L):\n",
        "        A, cache = linear_forward(A, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation_used)\n",
        "        caches.append(cache)\n",
        "\n",
        "    A, cache = linear_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], 'sigmoid')\n",
        "    caches.append(cache)\n",
        "\n",
        "    return A, caches\n",
        "\n",
        "  # Defining the cost function.\n",
        "  def cost(self, yhat, y):\n",
        "    m = y.shape[1]\n",
        "    cost = -np.sum(y * np.log(yhat) + (1-y) * np.log(1-yhat)) / m\n",
        "    return cost\n",
        "  \n",
        "  # Defining the Back Progation Algorithm. You can skip this part for now.\n",
        "  def backward_propagation(self, yhat, y, caches, activation_used):\n",
        "\n",
        "    def linear_backward(dA, cache, activation):\n",
        "      linear_cache, activation_cache = cache\n",
        "      A, W, b = linear_cache\n",
        "\n",
        "      if activation == 'sigmoid':\n",
        "        dZ = sigmoid_derivative(dA, activation_cache)\n",
        "      elif activation == 'relu':\n",
        "        dZ = relu_derivative(dA, activation_cache)  \n",
        "        \n",
        "      A_prev = A\n",
        "      m = A_prev.shape[1]\n",
        "\n",
        "      dW = np.dot(dZ, A_prev.T) / m\n",
        "      db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "      dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "      return dA_prev, dW, db\n",
        "\n",
        "\n",
        "    L = len(caches)\n",
        "    grads = {}\n",
        "    y = y.reshape(yhat.shape)\n",
        "    dyhat = -(np.divide(y, yhat) - np.divide(1-y, 1-yhat))\n",
        "    dAL = dyhat\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dAL, current_cache, 'sigmoid')\n",
        "    for l in range(L-1)[::-1]:\n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_backward(grads[\"dA\" + str(l+1)], current_cache, activation_used)\n",
        "\n",
        "    return grads\n",
        "  \n",
        "  # Defining the Gradient Descent Algorithm. Learning Rate is set to default at 0.01\n",
        "  def gradient_descent(self, parameters, grads, learning_rate=0.01):\n",
        "    L = parameters.__len__() // 2\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "      parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "      parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvYNLk9ShLBU",
        "colab_type": "text"
      },
      "source": [
        "### Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItLKf51XgaTk",
        "colab_type": "code",
        "outputId": "de30aac9-29cb-4552-da61-f9d0c65aa559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "'''\n",
        "  Welcome to my Playground! Here, you can run the Neural Network and even tinker with the parameters to discover new combinations and reach higher accuracies!\n",
        "\n",
        "  IMPORTANT NOTE : \n",
        "    > If you want to change the parameters, you'll first need to signup and then fork this repl.\n",
        "      I can't help it, it's in 'repl.it terms'.\n",
        "    > You don't need to change anything else. But you can if you wish to.\n",
        "    > The output visuals will be automatically saved in the Files Tab on the left of the screen. You      can access them from there.\n",
        "\n",
        "  Have fun!\n",
        "'''\n",
        "\n",
        "# # Importing helper functions\n",
        "# from network_functions import *\n",
        "# from helpers import *\n",
        "# import matplotlib as mpl\n",
        "\n",
        "# CHANGABLE PARAMETERS\n",
        "TRAINING_SAMPLES = 1000 # Total number of training samples.\n",
        "LAYER_DIMS = [2, 1]     # Note that the input Layer is predefined so you don't need to define it again.\n",
        "EPOCHS = 2500           # Total number of Iterations.\n",
        "LEARNING_RATE = 0.04    # Learning Rate to be used in Gradiend Descent.\n",
        "ACTIVATION = 'relu'     # Activations used in Neural Network. Try jumping between relu/sigmoid\n",
        "\n",
        "print(f'''\n",
        "Training Samples : {TRAINING_SAMPLES}\n",
        "Layer Dimensions : {LAYER_DIMS}\n",
        "Epochs           : {EPOCHS}\n",
        "Learning Rate    : {LEARNING_RATE}\n",
        "''')\n",
        "\n",
        "# Creating Data\n",
        "X, y = create_data(TRAINING_SAMPLES, 100)\n",
        "plot_data([X, y], 'Dataset')\n",
        "\n",
        "# Initializing Random Parameters\n",
        "parameters = initialize_random_parameters(LAYER_DIMS, X)\n",
        "\n",
        "# Few logs just to keep track of our training.\n",
        "cost_log, epoch_log = [], []\n",
        "\n",
        "# Creating the Network\n",
        "nn = NeuralNetwork()\n",
        "\n",
        "# Training\n",
        "print(\"Initializing Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  if epoch % 100 == 0 and epoch != 0:\n",
        "    LEARNING_RATE -= LEARNING_RATE/10     # This is called Learning Rate Decay. It is basically done to optimize our Training.\n",
        "    print(\"Epoch :\", epoch)\n",
        "  \n",
        "  # Feedforwarding\n",
        "  yhat, caches = nn.feedforward(X, parameters, ACTIVATION)\n",
        "  # Computing and saving the logs for plotting\n",
        "  cost = nn.cost(yhat, y)\n",
        "  cost_log.append(cost)\n",
        "  epoch_log.append(epoch+1)\n",
        "  \n",
        "  # Back Propagation\n",
        "  grads = nn.backward_propagation(yhat, y, caches, ACTIVATION)\n",
        "  # Gradient Descent\n",
        "  parameters = nn.gradient_descent(parameters, grads, LEARNING_RATE)\n",
        "  \n",
        "  \n",
        "predictions = yhat  # yhat --> the predicted output\n",
        "\n",
        "print()\n",
        "print(\"********** Accuracy :\", accuracy_score(predictions, y), \"% **********\")\n",
        "print(\"// Graphs saved. Check the files tab.\")\n",
        "\n",
        "# Saving the Cost Function Graph\n",
        "plot_cost_function(epoch_log, cost_log)\n",
        "\n",
        "# Just another way to convert predictions to 0s and 1s\n",
        "yhat = np.where(predictions<0.5, 0, 1)\n",
        "\n",
        "# Saving our Predictions graph\n",
        "plot_data([X, yhat], 'Prediction')\n",
        "\n",
        "\n",
        "'''\n",
        "  Awesome! You just created your First Neural Network from Scratch!\n",
        "\n",
        "  NOTE:\n",
        "    > You might not be getting an amazing accuracy. That's because there are various things that we've skipped and various parameters that we haven't optimized just to not go beyond the scope of this article.\n",
        "\n",
        "  Though, you can try to tinker with the 3 parameters:\n",
        "    > TRAINING_SAMPLES\n",
        "    > LAYER_DIMS\n",
        "    > EPOCHS\n",
        "    > LEARNING_RATE\n",
        "\n",
        "  Lemmi hear your adventures and accuracies through your comments!\n",
        "  Peace out.\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Samples : 1000\n",
            "Layer Dimensions : [2, 1]\n",
            "Epochs           : 2500\n",
            "Learning Rate    : 0.04\n",
            "\n",
            "Initializing Training...\n",
            "Epoch : 100\n",
            "Epoch : 200\n",
            "Epoch : 300\n",
            "Epoch : 400\n",
            "Epoch : 500\n",
            "Epoch : 600\n",
            "Epoch : 700\n",
            "Epoch : 800\n",
            "Epoch : 900\n",
            "Epoch : 1000\n",
            "Epoch : 1100\n",
            "Epoch : 1200\n",
            "Epoch : 1300\n",
            "Epoch : 1400\n",
            "Epoch : 1500\n",
            "Epoch : 1600\n",
            "Epoch : 1700\n",
            "Epoch : 1800\n",
            "Epoch : 1900\n",
            "Epoch : 2000\n",
            "Epoch : 2100\n",
            "Epoch : 2200\n",
            "Epoch : 2300\n",
            "Epoch : 2400\n",
            "\n",
            "********** Accuracy : 78.0 % **********\n",
            "// Graphs saved. Check the files tab.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  Awesome! You just created your First Neural Network from Scratch!\\n\\n  NOTE:\\n    > You might not be getting an amazing accuracy. That's because there are various things that we've skipped and various parameters that we haven't optimized just to not go beyond the scope of this article.\\n\\n  Though, you can try to tinker with the 3 parameters:\\n    > TRAINING_SAMPLES\\n    > LAYER_DIMS\\n    > EPOCHS\\n    > LEARNING_RATE\\n\\n  Lemmi hear your adventures and accuracies through your comments!\\n  Peace out.\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yqNhVC36HSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}